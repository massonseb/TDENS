{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "interstate-closer",
   "metadata": {},
   "source": [
    "## subject:\n",
    "In addition to the SST, I provided these datasets to complete the description of the El-Nino phenomenon. \n",
    " - precip.mon.mean.nc\n",
    " - slp.mnmean.nc\n",
    " - oaflux_nswrs.nc\n",
    " - u10_era5.nc\n",
    " - sla_allsat.nc\n",
    " \n",
    "Using basic xarray commands as seen in TD1, could you describe which variables are in these NetCDF files? \n",
    "What is the spatial and temporal domain and resolution? \n",
    "What is the origin of the data? Are they based on \"observations\" or models? Why do I use quotes in \"observations\"?\n",
    "\n",
    "Why did I choose these variables to illustrate El-Nino? Before doing any analyse/plot, which patterns do you expect to see in the interannual variability of these variables in the tropical Pacific? And why?\n",
    "\n",
    "Choose 1 atmospheric and 1 oceanic dataset among the 5th datasets.\n",
    "\n",
    "Following what was done in TD2, compute the regression and the correlation maps of these data on Nino 3.4 SST index.\n",
    "Explain the units of the maps and how to read them. What is the conceptual difference between these 2 maps?\n",
    "Give a physical interpretation of the results.\n",
    "\n",
    "Following what was done in TD3, compute the EOFS of 1 atmospheric and 1 oceanic dataset (change dataset if you can). Plot the % of explained variance by the first 10 EOFS. Plot the first 2 EOFS, and their PCs. Compute the correlation of the first 2 PCs with Nino3.4 SST index. Comment the results for the first EOF/PC. If you choose u10_era5.nc or slp.mnmean.nc as atmospheric data, you may exclude values outside of 25°S-25°N to exclude high-latitude signal and facilitate the analyse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-miniature",
   "metadata": {},
   "source": [
    "## Some advises/information for your pyhton scripts:\n",
    "\n",
    " - [There is a pdf file](https://github.com/massonseb/TDENS/blob/main/help_exam.pdf) which explains how to use this notebook to create your own notebook.\n",
    " - If you have problems with memory size limitation, reduce the memory footprint (see TD1) by extracting the data over a smaller domain (exclude high latitudes and for example parts of the Atlantic and/or Indian Oceans)\n",
    " - When using sel(lon=...) to select the longitude, check the longitude ranges (0 -> 360 or -180 -> 180) and values (0, 1,...) or (0.5, 1.5...). Same idea when selecting the latitude. In addition check if latitudes are defined in increasing or decreasing order.\n",
    " - Don't forget to look at dataset and variable attributes do get information about the datasets. Beware: to limit the size of the datasets, I regridded some of the datasets on a 1°x1° grid. Attributes related to the grid definition and resolution may be outdated -> look at the lon/lat values themselves\n",
    " - You will need to compute interannual anomalies of your dataset. You must first remove the linear trend on each point and next remove a monthly climatology (see TD1)\n",
    " - For the correlation and linear regression with Nino 3.4 (see TD2), make sure that you selected the common period between your dataset and Nino3.4 SST index.\n",
    " - The datasets do not have any mask file available to built the weights (as it was the case for the SST). Select 1 time step (any one) and plot a map of the data you chose to visualize it. Some data are defined everywhere, other have a specific value over the continents. Note that in some dataset, the specific value is also used to define areas covered by sea ice. These points change with time whereas continental points are fixed. You can simply exclude them by reducing the latitudinal domain (for example keep only 45°S-45°N).  \n",
    "  - If the data are defined everywhere, we will make things simpler and use all the points (over land and sea), which means that the weights will only depend on the latitude of the points:\n",
    "```python\n",
    "weights = np.cos( np.deg2rad(data.lat) )\n",
    "``` \n",
    "  - If the data have some specific value for continents, we will used them to build the land-sea mask. Note that, because of potential rounding errors, it is usually safer to test min or max values with a threshold value than testing the specific value itself. There is an example on how to proceed by testing the minimum value. \n",
    "```python\n",
    "min_acceptable_value = ...  # for example 0., -999., ...\n",
    "weights = data.XXX.isel(time=0) > min_acceptable_value # with data your Dataset and XXX your DataArray\n",
    "weights = weights * np.cos( np.deg2rad(data.lat) )\n",
    "``` \n",
    "In all cases, plot a map of your weights to check there is no errors!\n",
    "```python\n",
    "# if you want to keep only 25°S-25°N band uncomment the following line\n",
    "# weights = weights * ( abs(data.lat) < 25. )\n",
    "weights.plot()\n",
    "# note that this weights has no \"mask\" DataArray as it was the case for the weight we used for the SST\n",
    "# => you can directly use weight instead of weight.mask, e.g: solver = eof(XXXanom, weights=weights)\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-disorder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
